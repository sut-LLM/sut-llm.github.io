---
layout: page
parent: Lectures
title: Introduction to foundation models
nav_order: 1
---

# Lecture 1: Introduction to foundation models
{:.no_toc}

## Table of contents
{: .no_toc .text-delta }

1. TOC
{:toc}

---

## Autoregressive Language Models

- **Autoregressive Language Models** are a class of language models that generate text by predicting the next word in a sequence based on previous words.
- These models have played a pivotal role in natural language processing (NLP) and have evolved significantly over time.

## History of Language Models

- We'll explore the historical progression of language models, including:
  - **N-gram Models**: Early statistical models based on counting word sequences.
  - **Neural Language Models (Neural LM)**: Introduction of neural networks to language modeling.
  - **Recurrent Neural Networks (RNN)**: The use of RNNs for sequence modeling.

## Transformer Language Models

- **Transformer Architecture**: An innovative architecture that revolutionized NLP.
- Discussion of various Transformer-based language models, including:
  - **Encoder-Only Models**: Models that encode information but don't decode it.
  - **Encoder-Decoder Models**: Models used in translation and summarization tasks.
  - **Decoder-Only Models**: Models designed for text generation.

## Training Cost Function: Maximum Likelihood Estimation (MLE)

- **Maximum Likelihood Estimation (MLE)** is a common cost function used in training language models.
- Explanation of MLE and its role in optimizing model parameters for text generation.

## Scaling Foundation Models

- Addressing the challenges of scaling foundation models for better performance.
- **Prompting**: Techniques for guiding model output by providing prompts.
- **Emergence**: Understanding how complex language behaviors emerge from pretraining.

## Pretraining and Parameter-Efficient Fine-Tuning

- **Pretraining**: Learning representations from a large corpus of text data.
- **Parameter-Efficient Fine-Tuning**: Techniques to adapt pretrained models for specific tasks efficiently.

## Applications and Risks

- Explore real-world applications of foundation models across various domains, including:
  - **Text Generation**: Creating human-like text and content.
  - **Sentiment Analysis**: Understanding emotions and opinions in text.
  - **Question Answering**: Answering questions based on textual information.
- Discussion of ethical and societal risks associated with foundation models, including bias and misinformation.

## Conclusion

- Recap of the key concepts covered in this introductory lecture.
- An overview of the exciting journey ahead in exploring foundation models and their applications.

## Additional Resources

- Provide references and recommended readings for further exploration of foundation models and their history.

---

**Next Lecture:** Deep Dive into Transformer Architecture
```

This Markdown lecture note provides an overview of foundational concepts in language models, including their history, the Transformer architecture, training techniques, scaling challenges, and practical applications, while also addressing potential ethical concerns.